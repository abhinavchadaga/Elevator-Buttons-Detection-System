{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from detectron2.config import LazyConfig\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, get_detection_dataset_dicts\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.structures import Instances, Boxes, pairwise_iou\n",
    "from detectron2.data.detection_utils import annotations_to_instances\n",
    "\n",
    "from data_utils import read_split_file, register_dataset\n",
    "from detection_pipeline import ElevatorDetector, ElevatorDetectorLazyConf\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"vit\"\n",
    "use_recovery = True\n",
    "dataset_name = \"mixed\"\n",
    "iou_thresh = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/01 02:45:26 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from models/segmentation_vit/model_best.pth ...\n",
      "\u001b[32m[03/01 02:45:28 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from models/recovery_vit/model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "if backbone == \"vit\":\n",
    "    cfg = LazyConfig.load(\"configs/mask_rcnn_vit_base.py\")\n",
    "    recovery_weights = (\n",
    "        \"models/recovery_vit/model_best.pth\" if use_recovery else None\n",
    "    )\n",
    "    pipeline = ElevatorDetectorLazyConf(cfg, recovery_weights=recovery_weights)\n",
    "else:\n",
    "    pipeline = ElevatorDetector(use_recovery=use_recovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "registering mixed dataset: 100%|██████████| 22/22 [00:00<00:00, 27.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/01 02:45:30 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|   label    | 423          |   button   | 423          |\n",
      "|            |              |            |              |\n",
      "|   total    | 846          |            |              |\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Register datasets\n",
    "datasets = read_split_file(f\"data/panels/{dataset_name}/split.txt\")\n",
    "for spl, im_paths in zip([\"train\", \"val\", \"test\"], datasets):\n",
    "    DatasetCatalog.register(\n",
    "        f\"{dataset_name}_{spl}\",\n",
    "        lambda im_paths=im_paths: register_dataset(im_paths),\n",
    "    )\n",
    "    MetadataCatalog.get(f\"{dataset_name}_{spl}\").set(\n",
    "        thing_classes=[\"label\", \"button\"], thing_colors=[(0, 255, 0), (0, 0, 255)]\n",
    "    )\n",
    "metadata = MetadataCatalog.get(f\"{dataset_name}_train\")\n",
    "\n",
    "testset = get_detection_dataset_dicts(f\"{dataset_name}_test\", filter_empty=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=24, image_height=1363, image_width=2047, fields=[gt_boxes: Boxes(tensor([[ 181.,   32.,  415.,  279.],\n",
      "        [ 188.,  387.,  421.,  626.],\n",
      "        [ 200.,  730.,  433.,  966.],\n",
      "        [ 215., 1065.,  446., 1290.],\n",
      "        [ 757., 1072.,  988., 1298.],\n",
      "        [1305., 1074., 1538., 1297.],\n",
      "        [ 749.,   32.,  992.,  274.],\n",
      "        [ 749.,  385.,  990.,  628.],\n",
      "        [ 752.,  737.,  989.,  973.],\n",
      "        [1316.,  740., 1552.,  973.],\n",
      "        [1323.,  391., 1561.,  635.],\n",
      "        [1329.,   38., 1566.,  284.],\n",
      "        [ 466.,   31.,  693.,  268.],\n",
      "        [ 471.,  393.,  694.,  620.],\n",
      "        [ 480.,  744.,  697.,  961.],\n",
      "        [ 490., 1078.,  705., 1293.],\n",
      "        [1041., 1084., 1258., 1295.],\n",
      "        [1042.,  747., 1265.,  966.],\n",
      "        [1046.,  396., 1271.,  623.],\n",
      "        [1048.,   39., 1275.,  270.],\n",
      "        [1619.,   57., 1843.,  284.],\n",
      "        [1610.,  406., 1827.,  627.],\n",
      "        [1600.,  749., 1813.,  962.],\n",
      "        [1585., 1077., 1794., 1290.]])), gt_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), gt_masks: PolygonMasks(num_instances=24)])\n"
     ]
    }
   ],
   "source": [
    "resize_aug = T.ResizeShortestEdge(short_edge_length=1024, max_size=1024)\n",
    "# for d in testset:\n",
    "d = testset[0]\n",
    "gt_instances: Instances = annotations_to_instances(d[\"annotations\"], (d[\"height\"], d[\"width\"]))\n",
    "print(gt_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread(d[\"file_name\"])\n",
    "height, width = original_img.shape[:2]\n",
    "img = resize_aug.get_transform(original_img).apply_image(original_img)\n",
    "img_tensor = torch.as_tensor(img.astype(\"float32\").transpose(2, 0, 1))\n",
    "input = {\n",
    "    \"image\": img_tensor,\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"img_path\": d[\"file_name\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions: Instances = pipeline([input])[0][\"instances\"].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = gt_instances[gt_instances.gt_classes == 0].gt_boxes\n",
    "gt_btns = gt_instances[gt_instances.gt_classes == 1].gt_boxes\n",
    "\n",
    "pred_labels = predictions[predictions.pred_classes == 0].pred_boxes\n",
    "pred_btns = predictions[predictions.pred_classes == 1].pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes(tensor([[ 181.,   32.,  415.,  279.],\n",
      "        [ 188.,  387.,  421.,  626.],\n",
      "        [ 200.,  730.,  433.,  966.],\n",
      "        [ 215., 1065.,  446., 1290.],\n",
      "        [ 757., 1072.,  988., 1298.],\n",
      "        [1305., 1074., 1538., 1297.],\n",
      "        [ 749.,   32.,  992.,  274.],\n",
      "        [ 749.,  385.,  990.,  628.],\n",
      "        [ 752.,  737.,  989.,  973.],\n",
      "        [1316.,  740., 1552.,  973.],\n",
      "        [1323.,  391., 1561.,  635.],\n",
      "        [1329.,   38., 1566.,  284.]]))\n",
      "Boxes(tensor([[ 752.5822, 1076.4617,  995.5629, 1298.4468],\n",
      "        [ 197.8863,  733.8450,  435.5687,  964.3733],\n",
      "        [ 213.0734, 1064.6841,  450.7564, 1289.7684],\n",
      "        [1320.4713,   36.7600, 1567.2487,  284.7813],\n",
      "        [1306.7620, 1073.2610, 1539.5475, 1292.0822],\n",
      "        [ 178.3348,   34.6388,  418.8491,  278.6847],\n",
      "        [ 747.1640,   27.7416,  994.8909,  273.0611],\n",
      "        [ 743.7171,  381.9838,  994.4982,  625.8149],\n",
      "        [1314.0166,  739.8699, 1553.0757,  973.5745],\n",
      "        [ 182.4760,  384.9702,  423.5211,  622.5320],\n",
      "        [ 750.2045,  735.4528,  994.8151,  970.2772],\n",
      "        [1317.7015,  388.7513, 1561.9095,  631.0253]]))\n"
     ]
    }
   ],
   "source": [
    "print(gt_labels)\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9616, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.9450, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9580, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9695, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.9310, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9611, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9603, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9410, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.9518, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9842,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9503],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9526, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "labels_iou = pairwise_iou(gt_labels, pred_labels)\n",
    "print(labels_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "scores_per_gt = torch.max(labels_iou, dim=1).values\n",
    "num_correct = torch.where(scores_per_gt > iou_thresh, 1, 0).sum()\n",
    "num_correct\n",
    "total_labels = len(gt_labels)\n",
    "print(num_correct / total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2_parseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "505468a88c1ccc73ec335d6b8576761d1fc0425330bffda0aca10ca99e64b10f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
